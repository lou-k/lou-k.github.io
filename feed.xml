<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="https://lou.dev/feed.xml" rel="self" type="application/atom+xml" /><link href="https://lou.dev/" rel="alternate" type="text/html" /><updated>2020-11-17T06:14:03-05:00</updated><id>https://lou.dev/</id><title type="html">Lou Kratz</title><subtitle>Lou Kratz is a research engineer who specializes in computer vision and machine learning.
</subtitle><entry><title type="html">Kedro 6 Months In</title><link href="https://lou.dev/blog/2020/kedro.md/" rel="alternate" type="text/html" title="Kedro 6 Months In" /><published>2020-11-06T00:00:00-05:00</published><updated>2020-11-06T00:00:00-05:00</updated><id>https://lou.dev/blog/2020/kedro.md</id><content type="html" xml:base="https://lou.dev/blog/2020/kedro.md/">I build AI software in two modes: experimentation and productization. During experimentation, I am trying to see _if_ modern technology will solve my problem. If it does, 
I move on to productization and build reliable data pipelines at scale.

This presents a cyclical dependency when it comes to data engineering, however: I need reliable, maintainable data engineering pipelines during experimentation, but I don't know _what_ that pipeline should do until I've completed my experiments. 
In the past I, and many data scientists I know, have just used an _ad-hoc_ combination of bash scripts and jupyter notebooks wrangling experimental data.
While this may have been the fastest way to get experimental results and model building, it's really a tech debt that has to be paid down the road.

# The Problem

Specifically, the ad-hoc approach to experimental data pipelines causes pain points around:

* **Reproducibility**: Ad-hoc experimentation structures puts you at risk of making results that others can't produce, which can lead to product down time if/when you need to update your approach. Simple mistakes like executing a notebook cell twice, or forgetting to seed a random number generator can usually be caught. But other, more difficult problems can occur, such as behavior changes between dependency versions.
* **Readability**: If you've ever come across another person's experimental code, you know it's hard to find where to start. Even documented projects might just say &quot;run x script, y notebook, etc&quot;, and it's often unclear where the data come from and if you're on the right track.  Similarly, code reviews for data science projects are often hard to read: it's often asking a lot for a reader to differentiate notebook code that is for data manipulation or just visualization.
* **Maintainability**: It's common during data science projects to do some exploritory analysis or generate early results, and then revise how your data is processed or gathered. This becomes difficult and tedious when all of these steps are an unstructured collection of notebooks or scripts. In other words, the pipeline is hard to maintain: updating or changing it requires you to keep track of the whole thing. 
* **Sharability**:  Ad-hoc collections of notebooks and bash scripts are also difficult for a team to work on concurrently. Each member had to ensure their notebooks were up-to-date (version control on notebooks is less than ideal), and that they had the correct copy of any intermediate data.

# Enter Kedro


A lot of the issues above aren't new to software engineering disciplines and have been largely solved in that space. 
This is where [Kedro](https://kedro.readthedocs.io/en/stable/) comes in. Kedro is a framework for building data-engineering pipelines whose structure forces you to follow good software engineering practices.
By using kedro in the experimentation phase of projects, we can build maintainable and reproducible data pipelines that produce consistent experimental results.

Specifically, kedro has you organize your data engineering code into one or more _pipelines_. Each pipeline consist of a number of _nodes_: a functional unit that takes some data sets and parameters as inputs and produces new data sets. 

This simple but strict project structure is augmented by their [_data catalog_](https://kedro.readthedocs.io/en/stable/05_data/01_data_catalog.html): a yaml file that specifies how and where the input and output data sets are to be persisted to either disk or a cloud data storage service such as S3.

I started using kedro about six months ago, and since then have leveraged it for forr different experimental data pipelines. Some of these pipelines were for building models that eventually were deployed to production, and some were collaborations with team memebers. Below, I'll discuss the good and bad things I've found with kedro and how it helped us create reproducible, maintainable data pipleines.


## The Good

* **Reproducibility**: I can't say enough good things here: they nailed it. Their dependency management took a bit getting used to but it forces a specific version of all dependencies, which is awesome. Also, the ability to just type `kedro install` and `kedro run` to execute the whole pipeline is fantastic. You still have to remember to seed random number generators, but even that is easy to rememebr if you put it in their `params.yml` file.
* **Function Isolation**: Kedro's fixed project structure encourages you to think about what logical steps are necessary for your pipeline, and write a single node for each step. As a result, each node tends to be short (in terms of lines of code) and specific (in terms of logic). This makes each node easy to write, test, and read later on.
* **Developer Parallelization**: The small nodes also make it easier for developers to work togethger concurrently. It's often easy to spot nodes that won't depend on each other, and they can be coded concurrenty by different people.
* **Intermediate Data**: Perhaps my favorite thing about Kedro is the data catlog: just add the name of an output data set to `catalog.yml` and BOOM, it'll be serialized to disk or your cloud data store. This makes it super easy to build up the pipeline: you work on one node, commit it, execute it, and save the results. It also comes in handy when working on a team: I can run an expensive node on a big GPU machine and save the results to s3, and another team memeber can simply start from there. It's all baked in.
* **Code Reusability**:  I'll admit I have _never_ re-used a notebook. At best I pulled up an old one to remind myself how I achieved some complex analysis, but even then I had to remember the intricices of the data. The isolation of nodes, however, make it easy to re-use them. Also, Kedro's support for [modular pipelines](https://kedro.readthedocs.io/en/stable/06_nodes_and_pipelines/02_pipelines.html#developing-modular-pipelines) (i.e., packaging a pipeline into a pip package) makes it simple to share common code. We've even started publishing repos pipelines we use for common tasks such as image processing.

## The Bad
While kedro has solved many of the quality challenges in experiemtnal data pipelines, we have noticed a few gotchas that required less than elegant work arounds:
* **Incremental Dataset** support exists for reading data, but it's lacking for writing datasets. This hit us a few times when we had a node that would take 8-10 hours to run. If it fails part of the way through, we lost the work since it wasn't serialized until the end. Similarly, if the result dataset won't fit in memory, there wasn't a good way to save partial results since the intermediate data set writer in kedro assumes all data in is in memory. [This github issue](https://github.com/quantumblacklabs/kedro/issues/499) may fix it, if the developers address it, but for now you have to manage partial results on your own.
* **Pipeline Growth**. Pipelines can quickly get hard to follow since the input and outputs are just named variables that may or may not exist in the data catalog. [Kedro Viz](https://github.com/quantumblacklabs/kedro-viz) helps with this, but it's a bit annoying to switch between the navigator and code. We've also started enforcing name consistency between the node names and their functions, as well as the dataset names in the pipeline and the argument names in the node fucntions. Finally, making more, smaller pipelines is also a good way to keep your sanity. While all of these techniques help you to mentally keep track, it's still the trade off you make for coding the pipelines by naming the inputs and outputs.
* **Visulation** isn't really considered much in kedro, and is the one thing I'd say notebooks still have a leg up on. Kedro makes it easy for you to load the kedro context in a notebook, however, so you can still fire one up to do some visualization. Ultimately, however, I'd love to see better support within kedro for producing a graphical report that gets persisted to the `08_reporting` layer. Right now we worked around this by making a node that renders a nodebook to disk, but it's a  hack at best. I'd love better support for generating a final, data-driven reports that can be versioned in the data catalog much like the intermediate data.

# Conclusion

So am I a kedro convert? You betcha. It's replaced the spider-web of bash scripts and python notebooks I used to use for my experimental data pipelines and model training, and enabled better collaboration among our teams.
It won't replace a fully productionized stream-based data pipeline for me, but it absoluletly makes sure my experimental pipelines are maintainable, reproducable, and shareable.</content><author><name></name></author><category term="kedro," /><category term="data-engineering" /><summary type="html">I build AI software in two modes: experimentation and productization. During experimentation, I am trying to see _if_ modern technology will solve my problem. If it does, I move on to productization and build reliable data pipelines at scale.</summary></entry><entry><title type="html">Kedro 6 Months In</title><link href="https://lou.dev/blog/2020/kedro/" rel="alternate" type="text/html" title="Kedro 6 Months In" /><published>2020-11-06T00:00:00-05:00</published><updated>2020-11-06T00:00:00-05:00</updated><id>https://lou.dev/blog/2020/kedro</id><content type="html" xml:base="https://lou.dev/blog/2020/kedro/">&lt;p&gt;I build AI software in two modes: experimentation and productization. During experimentation, I am trying to see if modern technology will solve my problem. If it does, I move on to productization and build reliable data pipelines at scale.&lt;/p&gt;

&lt;p&gt;This presents a cyclical dependency when it comes to data engineering. I need reliable and maintainable data engineering pipelines during experimentation, but don’t know what that pipeline should do until after I’ve completed the experiments. In the past, I and many data scientists I know have used an ad-hoc combination of bash scripts and Jupyter Notebooks to wrangle experimental data. While this may have been the fastest way to get experimental results and model building, it’s really a technical debt that has to be paid down the road.&lt;/p&gt;

&lt;h1 id=&quot;the-problem&quot;&gt;The Problem&lt;/h1&gt;

&lt;p&gt;Specifically, the ad-hoc approach to experimental data pipelines causes pain points around:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reproducibility&lt;/strong&gt;: Ad-hoc experimentation structures puts you at risk of making results that others can’t reproduce, which can lead to product downtime if or when you need to update your approach. Simple mistakes like executing a notebook cell twice or forgetting to seed a random number generator can usually be caught. But other, more insidious problems can occur, such as behavior changes between dependency versions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Readability&lt;/strong&gt;: If you’ve ever come across another person’s experimental code, you know it’s hard to find where to start. Even documented projects might just say “run x script, y notebook, etc”, and it’s often unclear where the data come from and if you’re on the right track. Similarly, code reviews for data science projects are often hard to read: it’s asking a lot for a reader to differentiate between notebook code for data manipulation and code for visualization.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Maintainability&lt;/strong&gt;: It’s common during data science projects to do some exploratory analysis or generate early results, and then revise how your data is processed or gathered. This becomes difficult and tedious when all of these steps are an unstructured collection of notebooks or scripts. In other words, the pipeline is hard to maintain: updating or changing it requires you to keep track of the whole thing.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Shareability&lt;/strong&gt;: Ad-hoc collections of notebooks and bash scripts are also difficult for a team to work on concurrently. Each member has to ensure their notebooks are up to date (version control on notebooks is less than ideal), and that they have the correct copy of any intermediate data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;enter-kedro&quot;&gt;Enter Kedro&lt;/h1&gt;

&lt;p&gt;A lot of the issues above aren’t new to the software engineering discipline and have been largely solved in that space. This is where &lt;a href=&quot;https://kedro.readthedocs.io/en/stable/&quot;&gt;Kedro&lt;/a&gt; comes in. Kedro is a framework for building data engineering pipelines whose structure forces you to follow good software engineering practices. By using Kedro in the experimentation phase of projects, I can build maintainable and reproducible data pipelines that produce consistent experimental results.&lt;/p&gt;

&lt;p&gt;Specifically, Kedro has you organize your data engineering code into one or more pipelines. Each pipeline consists of a number of nodes: a functional unit that takes some data sets and parameters as inputs and produces new data sets, models, or artifacts.&lt;/p&gt;

&lt;p&gt;This simple but strict project structure is augmented by their &lt;a href=&quot;https://kedro.readthedocs.io/en/stable/05_data/01_data_catalog.html&quot;&gt;&lt;em&gt;data catalog&lt;/em&gt;&lt;/a&gt;: a YAML file that specifies how and where the input and output data sets are to be persisted. The data sets can be stored either locally or in a cloud data storage service such as S3.&lt;/p&gt;

&lt;p&gt;I started using Kedro about six months ago, and since then have leveraged it for different experimental data pipelines. Some of these pipelines were for building models that eventually were deployed to production, and some were collaborations with team members. Below, I’ll discuss the good and bad things I’ve found with Kedro and how it helped me create reproducible, maintainable data pipelines.&lt;/p&gt;

&lt;h2 id=&quot;the-good&quot;&gt;The Good&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reproducibility&lt;/strong&gt;: I can’t say enough good things here: they nailed it. Their dependency management took a bit of getting used to but it forces a specific version on all dependencies, which is awesome. Also, the ability to just type &lt;code class=&quot;highlighter-rouge&quot;&gt;kedro install&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;kedro run&lt;/code&gt; to execute the whole pipeline is fantastic. You still have to remember to seed random number generators, but even that is easy to remember if you put it in their &lt;code class=&quot;highlighter-rouge&quot;&gt;params.yml&lt;/code&gt; file.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Function Isolation&lt;/strong&gt;: Kedro’s fixed project structure encourages you to think about what logical steps are necessary for your pipeline, and write a single node for each step. As a result, each node tends to be short (in terms of lines of code) and specific (in terms of logic). This makes each node easy to write, test, and read later on.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Developer Parallelization&lt;/strong&gt;: The small nodes also make it easier for developers to work together concurrently. It’s easy to spot nodes that won’t depend on each other, and they can be coded concurrently by different people.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Intermediate Data&lt;/strong&gt;: Perhaps my favorite thing about Kedro is the data catalog. Just add the name of an output data set to &lt;code class=&quot;highlighter-rouge&quot;&gt;catalog.yml&lt;/code&gt; and BOOM, it’ll be serialized to disk or your cloud data store. This makes it super easy to build up the pipeline: you work on one node, commit it, execute it, and save the results. It also comes in handy when working on a team. I can run an expensive node on a big GPU machine and save the results to S3, and another team member can simply start from there. It’s all baked in.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Code Re-usability&lt;/strong&gt;: I’ll admit I have never re-used a notebook. At best I pulled up an old one to remind myself how I achieved some complex analysis, but even then I had to remember the intricacies of the data. The isolation of nodes, however, makes it easy to re-use them. Also, Kedro’s support for &lt;a href=&quot;https://kedro.readthedocs.io/en/stable/06_nodes_and_pipelines/02_pipelines.html#developing-modular-pipelines&quot;&gt;modular pipelines&lt;/a&gt; (i.e., packaging a pipeline into a pip package) makes it simple to share common code. I’ve created modular pipelines for common tasks such as image processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-bad&quot;&gt;The Bad&lt;/h2&gt;

&lt;p&gt;While Kedro has solved many of the quality challenges in experimental data pipelines, I have noticed a few gotchas that required less than elegant work arounds:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Incremental Dataset&lt;/strong&gt;: This support exists for reading data, but it’s lacking for writing datasets. This affected me a few times when I had a node that would take 8-10 hours to run. I lost work if the node failed part of the way through. Similarly, if the result data set didn’t fit in memory, there wasn’t a good way to save incremental results since the writer in Kedro assumes all partitions are in memory. &lt;a href=&quot;https://github.com/quantumblacklabs/kedro/issues/499&quot;&gt;This GitHub issue&lt;/a&gt; may fix it if the developers address it, but for now you have to manage partial results on your own.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pipeline Growth&lt;/strong&gt;: Pipelines can quickly get hard to follow since the input and outputs are just named variables that may or may not exist in the data catalog. &lt;a href=&quot;https://github.com/quantumblacklabs/kedro-viz&quot;&gt;Kedro Viz&lt;/a&gt; helps with this, but it’s a bit annoying to switch between the navigator and code. I’ve also started enforcing name consistency between the node names and their functions, as well as the data set names in the pipeline and the argument names in the node functions. Finally, making more, smaller pipelines is also a good way to keep your sanity. While all of these techniques help you to mentally keep track, it’s still the trade off you make for coding the pipelines by naming the inputs and outputs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Visualization&lt;/strong&gt;: This isn’t really considered much in Kedro, and is the one thing I’d say notebooks still have a leg up on. Kedro makes it easy for you to load the Kedro context in a notebook, however, so you can still fire one up to do some visualization. Ultimately, though, I’d love to see better support within Kedro for producing a graphical report that gets persisted to the &lt;code class=&quot;highlighter-rouge&quot;&gt;08_reporting&lt;/code&gt; layer. Right now I worked around this by making a node that renders a notebook to disk, but it’s a hack at best. I’d love better support for generating final, highly visual reports that can be versioned in the data catalog much like the intermediate data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;So am I a Kedro convert? Yah, you betcha. It replaces the spider-web of bash scripts and Python notebooks I used to use for my experimental data pipelines and model training, and enables better collaboration among our teams. It won’t replace a fully productionalized stream-based data pipeline for me, but it absolutely makes sure my experimental pipelines are maintainable, reproducible, and shareable.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I wrote this post for the &lt;a href=&quot;https://blog.developer.bazaarvoice.com/2020/11/16/kedro-6-months-in/&quot;&gt;Bazaarvoice Engineering Blog&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;</content><author><name></name></author><category term="kedro," /><category term="data-engineering" /><summary type="html">I build AI software in two modes: experimentation and productization. During experimentation, I am trying to see if modern technology will solve my problem. If it does, I move on to productization and build reliable data pipelines at scale.</summary></entry><entry><title type="html">Compressing Recorded Lectures with CRF and ffmpeg</title><link href="https://lou.dev/blog/2020/class-ffmpeg/" rel="alternate" type="text/html" title="Compressing Recorded Lectures with CRF and ffmpeg" /><published>2020-07-06T00:00:00-04:00</published><updated>2020-07-06T00:00:00-04:00</updated><id>https://lou.dev/blog/2020/class-ffmpeg</id><content type="html" xml:base="https://lou.dev/blog/2020/class-ffmpeg/">&lt;p&gt;Last spring I was teaching Introduction to Computer Vision at Drexel when COVID-19 hit and, well, everyone had to adjust quickly. While my course always had an online section, I decided to make all of my lectures asynchronous in to provide maximum flexibility to students (and myself).&lt;/p&gt;

&lt;p&gt;After some prep, I booted up quicktime on my mac and screen-recorded my first lecture at 1080p. Even though quicktime encoded using H.264, the file size was &lt;em&gt;massive&lt;/em&gt;: nearly 5 GB for only an hour lecture.&lt;/p&gt;

&lt;p&gt;No problem, I figured, probably just a high bit rate and lack of b-frames. I re-encoded it using two-pass x264 but found the result was still hundreds of megabytes.&lt;/p&gt;

&lt;p&gt;You might be wondering why this is a problem. Most videos are hundreds of megabytes, right? Well, I had two issues here:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I needed to ensure all of my students could watch the videos, and knew many of them may be in locations with poor internet access.&lt;/li&gt;
  &lt;li&gt;A lecture was &lt;em&gt;only&lt;/em&gt; my slides. I average about 1 per minute, so surely I only needed 60 frames of video which should be very, very small.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After some googling around, I found x264 had exactly a feature for such videos: &lt;a href=&quot;https://trac.ffmpeg.org/wiki/Encode/H.264&quot;&gt;Constant Rate Factor&lt;/a&gt; (CRF) encoding.  With CRF you instruct the encoder to target a provided quality level throughout the encoding. This is great for low-motion videos like lecture slides. Since there is very little movement the CRF encoder can keep a very low bitrate and result in an extremely small file size. I have the occasional video or animation in my slides and CRF simply increases the bitrates at those times.&lt;/p&gt;

&lt;p&gt;On top of that, I implemented a few more tricks such as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Telling x264 to optimize still images&lt;/li&gt;
  &lt;li&gt;Reducing the frame rate to 10fps&lt;/li&gt;
  &lt;li&gt;Adding fast start flags to the output file to make streaming easier and ensure compatibility with different devices.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My final ffmpeg command is:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ffmpeg -i INPUT.mov -pix_fmt yuv420p -c:v libx264 -crf 18 -preset veryslow -tune stillimage -r 10 -acodec aac -b:v 48k -f mp4  -movflags +faststart OUTPUT.mp4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This resulting files average less than a MB per minute, most of which is audio, but have hardly any artifacts in the video.&lt;/p&gt;</content><author><name></name></author><summary type="html">Last spring I was teaching Introduction to Computer Vision at Drexel when COVID-19 hit and, well, everyone had to adjust quickly. While my course always had an online section, I decided to make all of my lectures asynchronous in to provide maximum flexibility to students (and myself).</summary></entry><entry><title type="html">AI All Around How Curalate Accelerates Your Workflow</title><link href="https://lou.dev/blog/2018/ai-curalate/" rel="alternate" type="text/html" title="AI All Around How Curalate Accelerates Your Workflow" /><published>2018-10-09T00:00:00-04:00</published><updated>2018-10-09T00:00:00-04:00</updated><id>https://lou.dev/blog/2018/ai-curalate</id><content type="html" xml:base="https://lou.dev/blog/2018/ai-curalate/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Uploading EC2 Logs to S3 on Shutdown</title><link href="https://lou.dev/blog/2018/e2-logs/" rel="alternate" type="text/html" title="Uploading EC2 Logs to S3 on Shutdown" /><published>2018-09-04T00:00:00-04:00</published><updated>2018-09-04T00:00:00-04:00</updated><id>https://lou.dev/blog/2018/e2-logs</id><content type="html" xml:base="https://lou.dev/blog/2018/e2-logs/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">How Curalate uses MXNet on AWS for Deep Learning Magic</title><link href="https://lou.dev/blog/2018/mxnet/" rel="alternate" type="text/html" title="How Curalate uses MXNet on AWS for Deep Learning Magic" /><published>2018-07-27T00:00:00-04:00</published><updated>2018-07-27T00:00:00-04:00</updated><id>https://lou.dev/blog/2018/mxnet</id><content type="html" xml:base="https://lou.dev/blog/2018/mxnet/"></content><author><name></name></author><category term="computer" /><category term="vision" /><category term="deep" /><category term="machine" /><category term="learning" /><category term="curalate" /><category term="mxnet" /><category term="agile" /><summary type="html"></summary></entry><entry><title type="html">R&amp;amp;D At Curalate: A Case Study of Deep Metric Embedding</title><link href="https://lou.dev/blog/2018/deep-metric-embedding/" rel="alternate" type="text/html" title="R&amp;D At Curalate: A Case Study of Deep Metric Embedding" /><published>2018-02-01T05:11:36-05:00</published><updated>2018-02-01T05:11:36-05:00</updated><id>https://lou.dev/blog/2018/deep-metric-embedding</id><content type="html" xml:base="https://lou.dev/blog/2018/deep-metric-embedding/"></content><author><name></name></author><category term="computer" /><category term="vision" /><category term="deep" /><category term="metric" /><category term="machine" /><category term="learning" /><summary type="html"></summary></entry><entry><title type="html">Content Based Intelligent Cropping</title><link href="https://lou.dev/blog/2017/content-based-intelligent-cropping/" rel="alternate" type="text/html" title="Content Based Intelligent Cropping" /><published>2017-04-13T01:11:36-04:00</published><updated>2017-04-13T01:11:36-04:00</updated><id>https://lou.dev/blog/2017/content-based-intelligent-cropping</id><content type="html" xml:base="https://lou.dev/blog/2017/content-based-intelligent-cropping/"></content><author><name></name></author><category term="python" /><category term="notebook" /><category term="vision" /><category term="intelligent" /><category term="cropping" /><category term="smart" /><category term="product" /><category term="face" /><summary type="html"></summary></entry><entry><title type="html">Brewing EmojiNet</title><link href="https://lou.dev/blog/2016/emojinet/" rel="alternate" type="text/html" title="Brewing EmojiNet" /><published>2016-01-20T10:11:36-05:00</published><updated>2016-01-20T10:11:36-05:00</updated><id>https://lou.dev/blog/2016/emojinet</id><content type="html" xml:base="https://lou.dev/blog/2016/emojinet/"></content><author><name></name></author><category term="deep" /><category term="learning" /><category term="vision" /><category term="emoji" /><category term="convolutional" /><category term="neural" /><category term="network" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lou.dev/assets/2016-01-13-Emojinet/demo02.png" /></entry><entry><title type="html">Meet the Emojini 3000 – The Internet’s Premier Emoji Granting Genie</title><link href="https://lou.dev/blog/2015/emojini/" rel="alternate" type="text/html" title="Meet the Emojini 3000 – The Internet’s Premier Emoji Granting Genie" /><published>2015-12-17T00:00:00-05:00</published><updated>2015-12-17T00:00:00-05:00</updated><id>https://lou.dev/blog/2015/emojini</id><content type="html" xml:base="https://lou.dev/blog/2015/emojini/"></content><author><name></name></author><category term="emoji" /><category term="deep" /><category term="learning" /><category term="instagram" /><summary type="html"></summary></entry></feed>