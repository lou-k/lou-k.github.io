<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://lou.dev/feed.xml" rel="self" type="application/atom+xml" /><link href="https://lou.dev/" rel="alternate" type="text/html" /><updated>2020-12-11T06:36:23-05:00</updated><id>https://lou.dev/feed.xml</id><title type="html">Lou Kratz</title><subtitle>Lou Kratz is a research engineer who specializes in computer vision and machine learning.
</subtitle><entry><title type="html">Introducing PiPhoto</title><link href="https://lou.dev/blog/2020/piphoto/" rel="alternate" type="text/html" title="Introducing PiPhoto" /><published>2020-12-10T00:00:00-05:00</published><updated>2020-12-10T00:00:00-05:00</updated><id>https://lou.dev/blog/2020/piphoto</id><content type="html" xml:base="https://lou.dev/blog/2020/piphoto/">&lt;p&gt;I spend nearly every weekend with my SLR camera, but the last thing I want to do when I get home is pull out my computer to upload, organize, and backup the photos.
I just feels like &lt;em&gt;work&lt;/em&gt;. I know newer cameras have WiFi and all that, but my trusty D3300 isn’t going to be replaced anytime soon.
That’s where I got the idea for PiPhoto.&lt;/p&gt;

&lt;h1 id=&quot;piphoto&quot;&gt;PiPhoto&lt;/h1&gt;
&lt;p&gt;PiPhoto makes your raspberry pi automatically upload your photos when you insert your SD card.&lt;/p&gt;

&lt;p&gt;Here is a video showing PiPhoto uploading photos to OSX:&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/_iZTsNLnoRM&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;In this post, I’ll discuss the core components and challenges I faced. I’ve open-sourced the project on &lt;a href=&quot;https://github.com/lou-k/pi-photo-sync&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;mounting-the-sd-card-on-insert&quot;&gt;Mounting the SD Card on Insert&lt;/h2&gt;
&lt;p&gt;The first challenge I faced was how to automatically mount the sd card when it is inserted inserted into the Raspberry Pi.
&lt;a href=&quot;https://opensource.com/article/18/11/udev&quot;&gt;UDev&lt;/a&gt; is the commonly accepted way to do this. 
With some help from the &lt;a href=&quot;https://wiki.archlinux.org/index.php/udev#Mounting_drives_in_rules&quot;&gt;Arch Wiki&lt;/a&gt;, I settled upon this rule:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;99-mediastorage_card_instert_run.rules&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ACTION==&quot;add&quot;, SUBSYSTEMS==&quot;usb&quot;, SUBSYSTEM==&quot;block&quot;, ENV{ID_FS_USAGE}==&quot;filesystem&quot;, RUN{program}+=&quot;/usr/bin/systemd-mount --no-block --automount=no $devnode /media&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This tells the system to mount any filesystem block device to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/media&lt;/code&gt; when it is inserted into the USB port. Just pop that file into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/udev/rules.d/&lt;/code&gt; and you’re good to go.&lt;/p&gt;

&lt;h2 id=&quot;triggering-a-command-on-mount&quot;&gt;Triggering A Command on Mount&lt;/h2&gt;
&lt;p&gt;Once the sd card is mounted, I wanted to trigger a command to be run that would upload the photos to my computer, cloud, whatever. Though &lt;a href=&quot;https://www.freedesktop.org/wiki/Software/systemd/&quot;&gt;systemd&lt;/a&gt; is controversial, I found it was pretty easy to write a quick service that did just that:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;piphoto.service&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Unit]
Description=PiPhoto Trigger on Usb Mount
Requires=media.mount
After=media.mount

[Service]
ExecStart=/usr/local/bin/piphoto
Type=oneshot

[Install]
WantedBy=media.mount
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This systemd service triggers the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;piphoto&lt;/code&gt; command to execute when the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/media&lt;/code&gt; directory is mounted.&lt;/p&gt;

&lt;h2 id=&quot;un-mounting-when-the-job-is-finished&quot;&gt;Un-Mounting When The Job is Finished&lt;/h2&gt;
&lt;p&gt;Once the media are uploaded, I wanted the pi to unmount the sd card. This actually turned out to be a bit tricky: If the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;piphoto&lt;/code&gt; command called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unmount&lt;/code&gt;, then systemd would kill the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;piphoto.service&lt;/code&gt;, and it would report an error. Alternatively, if you have the systemd service call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;umount&lt;/code&gt; after exit using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ExecStartPost&lt;/code&gt;, then it catches the cyclic logic and throws an error.&lt;/p&gt;

&lt;p&gt;My solution was to have the systemd process unmount the drive, but do so in a different process. A quick and easy way to achieve this was to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;at&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Service]
ExecStart=/usr/local/bin/piphoto
Type=oneshot
ExecStartPost=/bin/bash -c 'echo &quot;systemd-umount /media&quot; | at now'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is a bit of a hack, so I’m open to better solutions :D.&lt;/p&gt;

&lt;h2 id=&quot;adding-led-feedback&quot;&gt;Adding LED Feedback&lt;/h2&gt;
&lt;p&gt;Okay, so now I have the pi:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;mounting the sd card when inserted&lt;/li&gt;
  &lt;li&gt;executing the piphoto command&lt;/li&gt;
  &lt;li&gt;umounting the sd card when complete&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next, I wanted to add some user feedback so I know if the job failed. To do this, I leveraged the pi’s on-board LEDs.&lt;/p&gt;

&lt;p&gt;As noted in the &lt;a href=&quot;https://mlagerberg.gitbooks.io/raspberry-pi/content/5.2-leds.html&quot;&gt;Pi Docs&lt;/a&gt;, you can change how the LED behaves by sending a string to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/sys/class/leds/led{number}/trigger&lt;/code&gt;.  There are a number of different options such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;timer&lt;/code&gt; to flash the led and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;default-on&lt;/code&gt; to keep it steady.&lt;/p&gt;

&lt;p&gt;First, I made the green light flash while the photos were uploading:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;echo timer | tee /sys/class/leds/led0/trigger
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and solid green when the syncing was complete:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;echo 'default-on' | tee  /sys/class/leds/led0/trigger &amp;gt; /dev/null
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, I wanted the red led to flash if the sync command failed.  Putting this all together into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pihole&lt;/code&gt; script:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
# start flashing the green led
echo timer | tee /sys/class/leds/led0/trigger

# upload the files
rsync -rav -e ssh /media ...
sync_status=$?

# Indicate success or failure
if [ $sync_status -eq 0 ] ; then
    # sync successful
    echo 'default-on' | tee  /sys/class/leds/led0/trigger &amp;gt; /dev/null
else
    # sync failure
    echo 'none' | tee /sys/class/leds/led0/trigger &amp;gt; /dev/null
    echo timer | tee /sys/class/leds/led1/trigger &amp;gt; /dev/null
fi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;using-in-practice&quot;&gt;Using in Practice&lt;/h2&gt;

&lt;p&gt;Above I’ve detailed the basics of how &lt;a href=&quot;https://github.com/lou-k/pi-photo-sync&quot;&gt;PiPhoto&lt;/a&gt; works. In the open-source release, you can easily change the sync command to fit your needs. I’ve included guides for:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/lou-k/pi-photo-sync/tree/main/destinations/osx-lightroom-classic&quot;&gt;Uploading to OSX for use with Lightroom Classic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/lou-k/pi-photo-sync/tree/main/destinations/ssh-copy-and-organize&quot;&gt;Organizing By Date While Copying Over SSH&lt;/a&gt; which I discussed in &lt;a href=&quot;http://localhost:4000/blog/2020/copy-existing-ssh/&quot;&gt;this blog post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, I added error handling and a Udev rule to reset the leds when you remove your SD card.&lt;/p&gt;

&lt;p&gt;I’ve already found PiPhoto to be a great convenience in my life. I hope you do too! If you find problems or have ideas, please hit up the &lt;a href=&quot;https://github.com/lou-k/pi-photo-sync/issues&quot;&gt;issue&lt;/a&gt; tracker on github.&lt;/p&gt;</content><author><name></name></author><summary type="html">I spend nearly every weekend with my SLR camera, but the last thing I want to do when I get home is pull out my computer to upload, organize, and backup the photos. I just feels like work. I know newer cameras have WiFi and all that, but my trusty D3300 isn’t going to be replaced anytime soon. That’s where I got the idea for PiPhoto.</summary></entry><entry><title type="html">Copy and Organizing Files Over Existing SSH Connections</title><link href="https://lou.dev/blog/2020/copy-existing-ssh/" rel="alternate" type="text/html" title="Copy and Organizing Files Over Existing SSH Connections" /><published>2020-12-04T00:00:00-05:00</published><updated>2020-12-04T00:00:00-05:00</updated><id>https://lou.dev/blog/2020/copy-existing-ssh</id><content type="html" xml:base="https://lou.dev/blog/2020/copy-existing-ssh/">&lt;p&gt;Often, I find myself copying files via ssh to a remote machine, and then shelling into that machine to organize them (or vice versa). A good example of this is how I organize my photos: I organize them into directories named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YYYY/YYYY-MM-DD/&lt;/code&gt; based on the exif data of the file. For example:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2020
├── 2020-11-01
│   ├── DSC_0536.NEF
│   ├── DSC_0537.NEF
│   ├── DSC_0538.NEF
├── 2020-11-07
│   ├── DSC_0001.NEF
│   ├── DSC_0002.NEF
│   ├── DSC_0003.NEF
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This presents a slight problem when copying over ssh. I either need to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Organize the files on my sd card before copying. I try to avoid moving a lot of data on the sd card to prolong it’s life.&lt;/li&gt;
  &lt;li&gt;Login twice via ssh (once to copy, and then again via an interactive shell to organize). Even with &lt;a href=&quot;https://www.ssh.com/ssh/key/&quot;&gt;ssh keys&lt;/a&gt;, this can be annoying if you use any 2FA method like &lt;a href=&quot;https://developers.yubico.com/PGP/SSH_authentication/&quot;&gt;yubikey&lt;/a&gt; or &lt;a href=&quot;https://duo.com/&quot;&gt;duo&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After some research, I found a more elegant solution: determine the image destination and then copy them through an &lt;em&gt;existing&lt;/em&gt; open ssh connection. Below, I detail how to achieve this.&lt;/p&gt;

&lt;h2 id=&quot;determining-the-photos-destination&quot;&gt;Determining the Photo’s Destination&lt;/h2&gt;

&lt;p&gt;First, I need to know in which directory the image should be copied to. &lt;a href=&quot;https://exiftool.org/&quot;&gt;exiftool&lt;/a&gt; can extract the date and time of when the picture was taken:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;exiftool &lt;span class=&quot;nt&quot;&gt;-T&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-createdate&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt;  &lt;span class=&quot;s2&quot;&gt;&quot;%Y-%m-%d&quot;&lt;/span&gt; image_file
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;That outputs the destination folder in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YYYY-MM-DD&lt;/code&gt; format.&lt;/p&gt;

&lt;p&gt;I’ll save that in a variable, and also extract the year:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;exiftool &lt;span class=&quot;nt&quot;&gt;-T&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-createdate&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt;  &lt;span class=&quot;s2&quot;&gt;&quot;%Y-%m-%d&quot;&lt;/span&gt; image_file&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;YEAR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--date&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DATE&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;+%Y&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;using-a-single-ssh-connection&quot;&gt;Using A Single SSH Connection&lt;/h2&gt;

&lt;p&gt;Next, I open an ssh connection using the &lt;a href=&quot;https://ldpreload.com/blog/ssh-control&quot;&gt;ControlMaster&lt;/a&gt; mode. When you connect to a remote host with this mode, future commands use the existing connection until the first connection is closed.&lt;/p&gt;

&lt;p&gt;There are two ways to use it. The first is to simply set the options when you form the first connection:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ControlPath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;~/.ssh/control-%h-%p-%r-o &lt;span class=&quot;nv&quot;&gt;ControlMaster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;yes &lt;/span&gt;user@host
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and then future connections can be made with:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ControlPath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;~/.ssh/control-%h-%p-%r user@host
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The second way is more elegant IMO. Edit your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.ssh/config&lt;/code&gt; file to set the control path for each host:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Host host
    User user
    HostName (IP/DNS)
    ControlPath ~/.ssh/control-%h-%p-%r
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and then making the first connection is rather easy to type:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh &lt;span class=&quot;nt&quot;&gt;-M&lt;/span&gt; host
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Future commands can be made with simply:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh host
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In both cases, you close the first connection with:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;putting-it-all-together&quot;&gt;Putting It All Together&lt;/h2&gt;

&lt;p&gt;Now, the logic to simultaneously copy and organize the files over ssh is pretty straightforward:&lt;/p&gt;

&lt;p&gt;First, open an ssh connection using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ControlMaster&lt;/code&gt; mode:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh -M host
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, for each image, we’ll:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Determine it’s destination folder&lt;/li&gt;
  &lt;li&gt;Ensure the destination exists using open ssh connection&lt;/li&gt;
  &lt;li&gt;Copy the image file using existing ssh connection&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;image_file &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[@]&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;# Determine it's destination folder&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;DATE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;exiftool &lt;span class=&quot;nt&quot;&gt;-T&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-createdate&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt;  &lt;span class=&quot;s2&quot;&gt;&quot;%Y-%m-%d&quot;&lt;/span&gt; image_file&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nv&quot;&gt;YEAR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--date&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DATE&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;+%Y&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Ensure the destination exists using open ssh connection&lt;/span&gt;
    ssh host &lt;span class=&quot;s2&quot;&gt;&quot;mkdir -p ./Pictures/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$YEAR&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DATE&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Copy the image&lt;/span&gt;
    scp image_file host:&lt;span class=&quot;s2&quot;&gt;&quot;./Pictures/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$YEAR&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DATE&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And finally close the ssh connection:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh &lt;span class=&quot;nt&quot;&gt;-O&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The ssh Control* features make it nice and easy to connect to a remote host and organize files while they are copied over. As noted by &lt;a href=&quot;https://ldpreload.com/blog/ssh-control&quot;&gt;Geoffrey Thomas
&lt;/a&gt;, it also speeds up subsequent file transfers since you don’t need to re-auth each time.&lt;/p&gt;

&lt;p&gt;I’ve packaged the logic discussed above into a bash script on &lt;a href=&quot;https://github.com/lou-k/pi-photo-sync/tree/main/destinations/ssh-copy-and-organize&quot;&gt;GitHub&lt;/a&gt;. Let me know if you use it!&lt;/p&gt;</content><author><name></name></author><summary type="html">Often, I find myself copying files via ssh to a remote machine, and then shelling into that machine to organize them (or vice versa). A good example of this is how I organize my photos: I organize them into directories named YYYY/YYYY-MM-DD/ based on the exif data of the file. For example: 2020 ├── 2020-11-01 │   ├── DSC_0536.NEF │   ├── DSC_0537.NEF │   ├── DSC_0538.NEF ├── 2020-11-07 │   ├── DSC_0001.NEF │   ├── DSC_0002.NEF │   ├── DSC_0003.NEF ...</summary></entry><entry><title type="html">Kedro 6 Months In</title><link href="https://lou.dev/blog/2020/kedro/" rel="alternate" type="text/html" title="Kedro 6 Months In" /><published>2020-11-06T00:00:00-05:00</published><updated>2020-11-06T00:00:00-05:00</updated><id>https://lou.dev/blog/2020/kedro</id><content type="html" xml:base="https://lou.dev/blog/2020/kedro/">&lt;p&gt;I build AI software in two modes: experimentation and productization. During experimentation, I am trying to see if modern technology will solve my problem. If it does, I move on to productization and build reliable data pipelines at scale.&lt;/p&gt;

&lt;p&gt;This presents a cyclical dependency when it comes to data engineering. I need reliable and maintainable data engineering pipelines during experimentation, but don’t know what that pipeline should do until after I’ve completed the experiments. In the past, I and many data scientists I know have used an ad-hoc combination of bash scripts and Jupyter Notebooks to wrangle experimental data. While this may have been the fastest way to get experimental results and model building, it’s really a technical debt that has to be paid down the road.&lt;/p&gt;

&lt;h1 id=&quot;the-problem&quot;&gt;The Problem&lt;/h1&gt;

&lt;p&gt;Specifically, the ad-hoc approach to experimental data pipelines causes pain points around:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reproducibility&lt;/strong&gt;: Ad-hoc experimentation structures puts you at risk of making results that others can’t reproduce, which can lead to product downtime if or when you need to update your approach. Simple mistakes like executing a notebook cell twice or forgetting to seed a random number generator can usually be caught. But other, more insidious problems can occur, such as behavior changes between dependency versions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Readability&lt;/strong&gt;: If you’ve ever come across another person’s experimental code, you know it’s hard to find where to start. Even documented projects might just say “run x script, y notebook, etc”, and it’s often unclear where the data come from and if you’re on the right track. Similarly, code reviews for data science projects are often hard to read: it’s asking a lot for a reader to differentiate between notebook code for data manipulation and code for visualization.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Maintainability&lt;/strong&gt;: It’s common during data science projects to do some exploratory analysis or generate early results, and then revise how your data is processed or gathered. This becomes difficult and tedious when all of these steps are an unstructured collection of notebooks or scripts. In other words, the pipeline is hard to maintain: updating or changing it requires you to keep track of the whole thing.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Shareability&lt;/strong&gt;: Ad-hoc collections of notebooks and bash scripts are also difficult for a team to work on concurrently. Each member has to ensure their notebooks are up to date (version control on notebooks is less than ideal), and that they have the correct copy of any intermediate data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;enter-kedro&quot;&gt;Enter Kedro&lt;/h1&gt;

&lt;p&gt;A lot of the issues above aren’t new to the software engineering discipline and have been largely solved in that space. This is where &lt;a href=&quot;https://kedro.readthedocs.io/en/stable/&quot;&gt;Kedro&lt;/a&gt; comes in. Kedro is a framework for building data engineering pipelines whose structure forces you to follow good software engineering practices. By using Kedro in the experimentation phase of projects, I can build maintainable and reproducible data pipelines that produce consistent experimental results.&lt;/p&gt;

&lt;p&gt;Specifically, Kedro has you organize your data engineering code into one or more pipelines. Each pipeline consists of a number of nodes: a functional unit that takes some data sets and parameters as inputs and produces new data sets, models, or artifacts.&lt;/p&gt;

&lt;p&gt;This simple but strict project structure is augmented by their &lt;a href=&quot;https://kedro.readthedocs.io/en/stable/05_data/01_data_catalog.html&quot;&gt;&lt;em&gt;data catalog&lt;/em&gt;&lt;/a&gt;: a YAML file that specifies how and where the input and output data sets are to be persisted. The data sets can be stored either locally or in a cloud data storage service such as S3.&lt;/p&gt;

&lt;p&gt;I started using Kedro about six months ago, and since then have leveraged it for different experimental data pipelines. Some of these pipelines were for building models that eventually were deployed to production, and some were collaborations with team members. Below, I’ll discuss the good and bad things I’ve found with Kedro and how it helped me create reproducible, maintainable data pipelines.&lt;/p&gt;

&lt;h2 id=&quot;the-good&quot;&gt;The Good&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reproducibility&lt;/strong&gt;: I can’t say enough good things here: they nailed it. Their dependency management took a bit of getting used to but it forces a specific version on all dependencies, which is awesome. Also, the ability to just type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kedro install&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kedro run&lt;/code&gt; to execute the whole pipeline is fantastic. You still have to remember to seed random number generators, but even that is easy to remember if you put it in their &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;params.yml&lt;/code&gt; file.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Function Isolation&lt;/strong&gt;: Kedro’s fixed project structure encourages you to think about what logical steps are necessary for your pipeline, and write a single node for each step. As a result, each node tends to be short (in terms of lines of code) and specific (in terms of logic). This makes each node easy to write, test, and read later on.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Developer Parallelization&lt;/strong&gt;: The small nodes also make it easier for developers to work together concurrently. It’s easy to spot nodes that won’t depend on each other, and they can be coded concurrently by different people.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Intermediate Data&lt;/strong&gt;: Perhaps my favorite thing about Kedro is the data catalog. Just add the name of an output data set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;catalog.yml&lt;/code&gt; and BOOM, it’ll be serialized to disk or your cloud data store. This makes it super easy to build up the pipeline: you work on one node, commit it, execute it, and save the results. It also comes in handy when working on a team. I can run an expensive node on a big GPU machine and save the results to S3, and another team member can simply start from there. It’s all baked in.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Code Re-usability&lt;/strong&gt;: I’ll admit I have never re-used a notebook. At best I pulled up an old one to remind myself how I achieved some complex analysis, but even then I had to remember the intricacies of the data. The isolation of nodes, however, makes it easy to re-use them. Also, Kedro’s support for &lt;a href=&quot;https://kedro.readthedocs.io/en/stable/06_nodes_and_pipelines/02_pipelines.html#developing-modular-pipelines&quot;&gt;modular pipelines&lt;/a&gt; (i.e., packaging a pipeline into a pip package) makes it simple to share common code. I’ve created modular pipelines for common tasks such as image processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-bad&quot;&gt;The Bad&lt;/h2&gt;

&lt;p&gt;While Kedro has solved many of the quality challenges in experimental data pipelines, I have noticed a few gotchas that required less than elegant work arounds:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Incremental Dataset&lt;/strong&gt;: This support exists for reading data, but it’s lacking for writing datasets. This affected me a few times when I had a node that would take 8-10 hours to run. I lost work if the node failed part of the way through. Similarly, if the result data set didn’t fit in memory, there wasn’t a good way to save incremental results since the writer in Kedro assumes all partitions are in memory. &lt;a href=&quot;https://github.com/quantumblacklabs/kedro/issues/499&quot;&gt;This GitHub issue&lt;/a&gt; may fix it if the developers address it, but for now you have to manage partial results on your own.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pipeline Growth&lt;/strong&gt;: Pipelines can quickly get hard to follow since the input and outputs are just named variables that may or may not exist in the data catalog. &lt;a href=&quot;https://github.com/quantumblacklabs/kedro-viz&quot;&gt;Kedro Viz&lt;/a&gt; helps with this, but it’s a bit annoying to switch between the navigator and code. I’ve also started enforcing name consistency between the node names and their functions, as well as the data set names in the pipeline and the argument names in the node functions. Finally, making more, smaller pipelines is also a good way to keep your sanity. While all of these techniques help you to mentally keep track, it’s still the trade off you make for coding the pipelines by naming the inputs and outputs.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Visualization&lt;/strong&gt;: This isn’t really considered much in Kedro, and is the one thing I’d say notebooks still have a leg up on. Kedro makes it easy for you to load the Kedro context in a notebook, however, so you can still fire one up to do some visualization. Ultimately, though, I’d love to see better support within Kedro for producing a graphical report that gets persisted to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;08_reporting&lt;/code&gt; layer. Right now I worked around this by making a node that renders a notebook to disk, but it’s a hack at best. I’d love better support for generating final, highly visual reports that can be versioned in the data catalog much like the intermediate data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;So am I a Kedro convert? Yah, you betcha. It replaces the spider-web of bash scripts and Python notebooks I used to use for my experimental data pipelines and model training, and enables better collaboration among our teams. It won’t replace a fully productionalized stream-based data pipeline for me, but it absolutely makes sure my experimental pipelines are maintainable, reproducible, and shareable.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I wrote this post for the &lt;a href=&quot;https://blog.developer.bazaarvoice.com/2020/11/16/kedro-6-months-in/&quot;&gt;Bazaarvoice Engineering Blog&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;</content><author><name></name></author><category term="kedro," /><category term="data-engineering" /><summary type="html">I build AI software in two modes: experimentation and productization. During experimentation, I am trying to see if modern technology will solve my problem. If it does, I move on to productization and build reliable data pipelines at scale.</summary></entry><entry><title type="html">Compressing Recorded Lectures with CRF and ffmpeg</title><link href="https://lou.dev/blog/2020/class-ffmpeg/" rel="alternate" type="text/html" title="Compressing Recorded Lectures with CRF and ffmpeg" /><published>2020-07-06T00:00:00-04:00</published><updated>2020-07-06T00:00:00-04:00</updated><id>https://lou.dev/blog/2020/class-ffmpeg</id><content type="html" xml:base="https://lou.dev/blog/2020/class-ffmpeg/">&lt;p&gt;Last spring I was teaching Introduction to Computer Vision at Drexel when COVID-19 hit and, well, everyone had to adjust quickly. While my course always had an online section, I decided to make all of my lectures asynchronous in to provide maximum flexibility to students (and myself).&lt;/p&gt;

&lt;p&gt;After some prep, I booted up quicktime on my mac and screen-recorded my first lecture at 1080p. Even though quicktime encoded using H.264, the file size was &lt;em&gt;massive&lt;/em&gt;: nearly 5 GB for only an hour lecture.&lt;/p&gt;

&lt;p&gt;No problem, I figured, probably just a high bit rate and lack of b-frames. I re-encoded it using two-pass x264 but found the result was still hundreds of megabytes.&lt;/p&gt;

&lt;p&gt;You might be wondering why this is a problem. Most videos are hundreds of megabytes, right? Well, I had two issues here:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I needed to ensure all of my students could watch the videos, and knew many of them may be in locations with poor internet access.&lt;/li&gt;
  &lt;li&gt;A lecture was &lt;em&gt;only&lt;/em&gt; my slides. I average about 1 per minute, so surely I only needed 60 frames of video which should be very, very small.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After some googling around, I found x264 had exactly a feature for such videos: &lt;a href=&quot;https://trac.ffmpeg.org/wiki/Encode/H.264&quot;&gt;Constant Rate Factor&lt;/a&gt; (CRF) encoding.  With CRF you instruct the encoder to target a provided quality level throughout the encoding. This is great for low-motion videos like lecture slides. Since there is very little movement the CRF encoder can keep a very low bitrate and result in an extremely small file size. I have the occasional video or animation in my slides and CRF simply increases the bitrates at those times.&lt;/p&gt;

&lt;p&gt;On top of that, I implemented a few more tricks such as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Telling x264 to optimize still images&lt;/li&gt;
  &lt;li&gt;Reducing the frame rate to 10fps&lt;/li&gt;
  &lt;li&gt;Adding fast start flags to the output file to make streaming easier and ensure compatibility with different devices.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My final ffmpeg command is:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ffmpeg -i INPUT.mov -pix_fmt yuv420p -c:v libx264 -crf 18 -preset veryslow -tune stillimage -r 10 -acodec aac -b:v 48k -f mp4  -movflags +faststart OUTPUT.mp4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This resulting files average less than a MB per minute, most of which is audio, but have hardly any artifacts in the video.&lt;/p&gt;</content><author><name></name></author><summary type="html">Last spring I was teaching Introduction to Computer Vision at Drexel when COVID-19 hit and, well, everyone had to adjust quickly. While my course always had an online section, I decided to make all of my lectures asynchronous in to provide maximum flexibility to students (and myself).</summary></entry><entry><title type="html">AI All Around How Curalate Accelerates Your Workflow</title><link href="https://lou.dev/blog/2018/ai-curalate/" rel="alternate" type="text/html" title="AI All Around How Curalate Accelerates Your Workflow" /><published>2018-10-09T00:00:00-04:00</published><updated>2018-10-09T00:00:00-04:00</updated><id>https://lou.dev/blog/2018/ai-curalate</id><content type="html" xml:base="https://lou.dev/blog/2018/ai-curalate/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Uploading EC2 Logs to S3 on Shutdown</title><link href="https://lou.dev/blog/2018/e2-logs/" rel="alternate" type="text/html" title="Uploading EC2 Logs to S3 on Shutdown" /><published>2018-09-04T00:00:00-04:00</published><updated>2018-09-04T00:00:00-04:00</updated><id>https://lou.dev/blog/2018/e2-logs</id><content type="html" xml:base="https://lou.dev/blog/2018/e2-logs/"></content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">How Curalate uses MXNet on AWS for Deep Learning Magic</title><link href="https://lou.dev/blog/2018/mxnet/" rel="alternate" type="text/html" title="How Curalate uses MXNet on AWS for Deep Learning Magic" /><published>2018-07-27T00:00:00-04:00</published><updated>2018-07-27T00:00:00-04:00</updated><id>https://lou.dev/blog/2018/mxnet</id><content type="html" xml:base="https://lou.dev/blog/2018/mxnet/"></content><author><name></name></author><category term="computer" /><category term="vision" /><category term="deep" /><category term="machine" /><category term="learning" /><category term="curalate" /><category term="mxnet" /><category term="agile" /><summary type="html"></summary></entry><entry><title type="html">R&amp;amp;D At Curalate: A Case Study of Deep Metric Embedding</title><link href="https://lou.dev/blog/2018/deep-metric-embedding/" rel="alternate" type="text/html" title="R&amp;amp;D At Curalate: A Case Study of Deep Metric Embedding" /><published>2018-02-01T05:11:36-05:00</published><updated>2018-02-01T05:11:36-05:00</updated><id>https://lou.dev/blog/2018/deep-metric-embedding</id><content type="html" xml:base="https://lou.dev/blog/2018/deep-metric-embedding/"></content><author><name></name></author><category term="computer" /><category term="vision" /><category term="deep" /><category term="metric" /><category term="machine" /><category term="learning" /><summary type="html"></summary></entry><entry><title type="html">Content Based Intelligent Cropping</title><link href="https://lou.dev/blog/2017/content-based-intelligent-cropping/" rel="alternate" type="text/html" title="Content Based Intelligent Cropping" /><published>2017-04-13T01:11:36-04:00</published><updated>2017-04-13T01:11:36-04:00</updated><id>https://lou.dev/blog/2017/content-based-intelligent-cropping</id><content type="html" xml:base="https://lou.dev/blog/2017/content-based-intelligent-cropping/"></content><author><name></name></author><category term="python" /><category term="notebook" /><category term="vision" /><category term="intelligent" /><category term="cropping" /><category term="smart" /><category term="product" /><category term="face" /><summary type="html"></summary></entry><entry><title type="html">Brewing EmojiNet</title><link href="https://lou.dev/blog/2016/emojinet/" rel="alternate" type="text/html" title="Brewing EmojiNet" /><published>2016-01-20T10:11:36-05:00</published><updated>2016-01-20T10:11:36-05:00</updated><id>https://lou.dev/blog/2016/emojinet</id><content type="html" xml:base="https://lou.dev/blog/2016/emojinet/"></content><author><name></name></author><category term="deep" /><category term="learning" /><category term="vision" /><category term="emoji" /><category term="convolutional" /><category term="neural" /><category term="network" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lou.dev/assets/2016-01-13-Emojinet/demo02.png" /><media:content medium="image" url="https://lou.dev/assets/2016-01-13-Emojinet/demo02.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>